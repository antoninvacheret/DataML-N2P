{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5563d398-cd21-4dd9-9f95-4c4827e69ecd",
   "metadata": {},
   "source": [
    "# Dimensionality reduction techniques comparison\n",
    "**[Kaciel Béraud](mailto:beraud@lpccaen.in2p3.fr)**, Laboratoire de Physique Corpusculaire de Caen, Caen, France.\n",
    "\n",
    "Date: **28 September 2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067787b-60d8-44db-9398-42ef48897867",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "This session will aim to learn how to play with some data and do some basic classification using classical methods using the [Scikit Learn](https://scikit-learn.org/stable/) and [UMAP learn](https://umap-learn.readthedocs.io/en/latest/) library.\n",
    "\n",
    "The objectives of this session is to be able to classify the a dataset into there respective class using different classification technics. You will play with some data to understand the technics and apply what you've learn to a new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9abbce1-f653-4dcc-879f-dedd8a0dc3da",
   "metadata": {},
   "source": [
    "## Required Libraries\n",
    "\n",
    "We suppose you still have the libraries from last lesson, today you will also needs:\n",
    "\n",
    "* [Seaborn](https://seaborn.pydata.org/) to plot some heatmaps.\n",
    "\n",
    "* [UMAP](https://umap-learn.readthedocs.io/en/latest/) to perform Uniform Manifold Approximation and Projection for Dimension Reduction. The package name is `umap-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456880de-a077-413a-a0e0-4f1122ac2f6e",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will use the Wine dataset from Scikitlearn (UCI). It's composed of 178 wine analysis from 3 different producer. \n",
    "\n",
    "Here's the list of the features available: \n",
    "\n",
    "* alcohol\n",
    "* malic_acid\n",
    "* ash\n",
    "* alcalinity_of_ash\n",
    "* magensium\n",
    "* total_phenols\n",
    "* flavanoids\n",
    "* nonflavanoid_phenols\n",
    "* proanthocyanins\n",
    "* color_intensity\n",
    "* hue\n",
    "* od280od315_of_diluted_wines\n",
    "* proline\n",
    "\n",
    "Over 3 different classes, 0, 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6c9eca-df20-4cb8-adf9-c72d6d94d4b7",
   "metadata": {},
   "source": [
    "One can load and take a look at the dataset using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4413b7-cce2-4fd9-9591-b62efd84068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # to show the dataframes nicely\n",
    "\n",
    "# You can load the data like\n",
    "# as_frame = True allow one to get a dataframe\n",
    "data = load_wine(as_frame = True)\n",
    "\n",
    "print(data.keys())\n",
    "\n",
    "frame = data[\"frame\"] # take the frame, which also contains the target class\n",
    "df = pd.DataFrame(frame) # put it as a pandas\n",
    "\n",
    "display(df)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c869de94-e701-4390-aae0-3a0f14ed1752",
   "metadata": {},
   "source": [
    "The loaded dataset is a dictionnary containing the data, features and some metadata. \n",
    "\n",
    "The `as_frame` argument allow to keep the column name, which is easier to represent, but for a computation purpose, we will not keep them.\n",
    "\n",
    "The data is stored using a numpy array, it contains the 13 features with 178 observations and we can convert it to a pandas dataframe to work on them.\n",
    "\n",
    "You can explore the dataset and try to find some correlation by hands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa22f483",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Explore the data by plotting 1 dimension histogram of the features.\n",
    "\n",
    "Use the `target` class to select along the group and look at the distribution with and without the selection.\n",
    "\n",
    "Can you find easy discrimination variable in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolor\n",
    "\n",
    "# Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2193f91",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Now try to use a second variable and plot 2 dimensions histogram of the features.\n",
    "\n",
    "Again, use the `target` class to select along the group and look at the distribution with and without the selection.\n",
    "\n",
    "Can you find better way of discriminating the data? Is it enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ce396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data, but in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc833aa-380b-4aa0-9e6e-8cb5885bc95a",
   "metadata": {},
   "source": [
    "# Dimension reduction\n",
    "\n",
    "We can manually discreminate up to 2 or even 3 dimensions, but since we have 13 of them, it's impossible to do by hands. That's why we can use **Dimension Reduction** technics, this consist in finding the distances between the points in a hyperspace. The methods can be viewed as geometric even if we are not in a 3D space\n",
    "\n",
    "For computation purpose, we do not keep the columns names and only get a numpy matrix. We also do not include the target in the features, which will defeat the purpose of the dimension reduction technics.\n",
    "\n",
    "Before we start, we can prepare a few visualization functions that will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f3b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the data to remove the labels name\n",
    "data = load_wine(as_frame = False)\n",
    "features = data[\"data\"]\n",
    "labels = data[\"target\"]\n",
    "features_names = data[\"feature_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3786920-09db-45f6-9b03-07a76b79dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#%matplotlib widget\n",
    "\n",
    "def visualize_2d(x, labels):\n",
    "    sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=labels, s=100, alpha=0.8,\n",
    "                    palette=\"viridis\", edgecolor=\"black\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_3d(x, labels, elev = 30, azim = 45, roll = 0):\n",
    "    # Workaround as axis limits are not auto-scaling\n",
    "    x_norm = (x - x.min(axis=0)) / (x.max(axis=0) - x.min(axis=0))\n",
    "    x, y, z = x_norm[:, 0], x_norm[:, 1], x_norm[:, 2]\n",
    "\n",
    "    # Colors\n",
    "    cmap = plt.get_cmap('viridis', 3)\n",
    "    color = cmap(labels)\n",
    "    fig = plt.figure(figsize = (7,7))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(x, y, z, marker=\"o\", c = color)\n",
    "    ax.view_init(elev = elev, azim = azim, roll = roll) # if roll give errors, just remove it from this line\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52089e-1940-4beb-b1f0-dcb63677a8f2",
   "metadata": {},
   "source": [
    "## Correlation matrix\n",
    "\n",
    "One of the first thing to understand a dataset is to compute the correlation coefficients matrix and then computing the eigenvectors and eigenvalues. This matrix will show the correlation between each features, it's an easy way to find which feature are corrolated with another one.\n",
    "\n",
    "This can be done manually or using numpy's functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519fed1a-f18d-4824-b600-cae29c6cc23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to transpose the matrix to get the correlation coefficient of the features\n",
    "cov = np.corrcoef(features.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov)\n",
    "plt.imshow(cov, vmin = -1, vmax = 1)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "print(\"Eigenvalues: \\n\", eig_vals)\n",
    "print(\"Eigenvectors: \\n\", eig_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519bb2d-99f6-4265-be3c-04c4990fd133",
   "metadata": {},
   "source": [
    "Note that we need to transpose the matrix, if you do not make the transpose, you will get the correlation coefficient of each observation with another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe42c1-4657-4926-a39b-89d9cfb168d5",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "The first method of dimension reduction is the PCA meaning \"Principal Component Analysis\". The data are linearly transform into a new coordinate system to capture the largest variation in the data.\n",
    "\n",
    "Here using Scikit Learn's decomposition submodule, we can use directly the PCA to look at our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77559795-d376-4835-8501-36debea122ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "pca_2d = pca.fit_transform(features)\n",
    "visualize_2d(x = pca_2d, labels = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b1395e-d3c3-41b3-ba77-293add7ea93f",
   "metadata": {},
   "source": [
    "Which you can also get the eigenvectors and eigenvalues of the PCA. We that the eignevalues correspond to the x and y dispersion.\n",
    "\n",
    "You also have the two component of the eigenvectors for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f231346-986d-40d9-945b-439cf38d8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Eigenvalues: \\n\", pca.explained_variance_ratio_)\n",
    "print(\"Eigenvectors: \\n\", pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b07ac-da9f-4b5f-84ea-0ee7914027e0",
   "metadata": {},
   "source": [
    "Setting the number of components of the PCA allow us to find the 2 most important features in the dataset, which are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428cd07-928e-4993-80d4-a72b480f6bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First most contributing component: '{features_names[np.argmax(pca.components_[0])]}' contributing at: , {pca.explained_variance_ratio_[0]*100:.2f}%\")\n",
    "print(f\"Second most contributing component: '{features_names[np.argmax(pca.components_[1])]}' contributing at: {pca.explained_variance_ratio_[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6894d4-9c97-4938-bc36-48526a9e5670",
   "metadata": {},
   "source": [
    "We can also increase the number of components to match the number of dimension, it will sort each component into there respective contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ca648-752d-41d7-a8da-b7a400039964",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 13)\n",
    "pca_2d = pca.fit_transform(features)\n",
    "print(\"Eigenvalues: \\n\", pca.explained_variance_ratio_)\n",
    "print(\"Eigenvectors: \\n\", pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16beebff-b2a4-4d4d-bab8-a3ce443fbfed",
   "metadata": {},
   "source": [
    "We can see that the amount of information carried by each dimension fall down very quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d617c0a-1810-4081-9467-a2fb706309d3",
   "metadata": {},
   "source": [
    "## 3D Projection\n",
    "\n",
    "We can represent data up to 3 dimension, using the same snippet of code, but we set the number of component to 3.\n",
    "\n",
    "With a third component, we might see a separation between the group, which we can visualized with a 3D scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2dc48b-17d1-4a6d-b8b6-c66658a6e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_3d = pca.fit_transform(features)\n",
    "visualize_3d(pca_3d, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ef853-c595-4088-a320-3e510a427acf",
   "metadata": {},
   "source": [
    "You can make the plot rotate by adding an angle, the first one is the elevation and the second one is the azymut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be83bf-fd5c-44fe-ac5e-cf02e1ce8fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_3d(pca_3d, labels, 0, 135)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1d332-0543-423d-8ff1-8ae35bdcee8f",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "After one **Dimension Reduction** analysis, you can add a new dimension in your data which can be a first classification to orientate future steps.\n",
    "\n",
    "This can be done using **NumPy's array** and a seperation function. In our case, we cannot discreminate easily between class 2 et 3, so we will just separate between class 0 and the other class;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05076cd3-0a5d-45c9-ae89-751ea9386075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separation_function(x, y) :\n",
    "    if x > 180 :\n",
    "        r_val = 0\n",
    "    else :\n",
    "        r_val = 1\n",
    "    return r_val\n",
    "\n",
    "l = []\n",
    "for pos in pca_2d : # we loop over each pair of points\n",
    "    l.append(separation_function(pos[0], pos[1]))\n",
    "\n",
    "np_l = np.array(l)\n",
    "features = np.c_[features, np_l]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d39d27-a62b-42e6-971f-5de4a912211c",
   "metadata": {},
   "source": [
    "`np.c_[]` allows you to append a column in a **NumPy's matrix** and `np.r_[]` is the row equivalent. Other functions like `np.concatenate` exists using a different syntax, you can choose between them depending on how you handle the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f32e6",
   "metadata": {},
   "source": [
    "After the classification, we can measure the quality of the separation by computing the number of **True positives** versus the number of **False positive**.\n",
    "\n",
    "In our case we can check the quality of the classification of the `class_0` versus the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of real class 0 observations: {np.count_nonzero(labels == 0)}\")\n",
    "print(f\"Number of separated class 0: {np.count_nonzero(features[:,-1] == 0)}\") # -1 is the last column\n",
    "print(f\"Number of True Positives: {np.count_nonzero((features[:,-1] == 0) & (labels == 0))}\")\n",
    "print(f\"Number of False Positives: {np.count_nonzero((features[:,-1] == 0) & (labels != 0))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d5187",
   "metadata": {},
   "source": [
    "We see that we only have 2 **False Positives** in the separation of the `class_0` but we still miss 13 observations that are excluded by our cut. When choosing a cut like that, you want to have the highest number of **True Positives** with the lowest amount of **False Positives** and the cut you will take is always a compromised between them.\n",
    "\n",
    "Doing this comparaison with a multitudes of cut will give you a **ROC curve**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27983be5-d630-4f1f-b6b7-e0fa835f2a81",
   "metadata": {},
   "source": [
    "## MDS\n",
    "\n",
    "MDS stands for **Multidimensional scaling**, it's another method of Dimension reduction, this time it's a non-linear one.\n",
    "\n",
    "This method take in input a matrix with the distances between each pair of objects. The result of the MDS will also be affected by the distance algorithm used.\n",
    "\n",
    "We start by computing the distance in a latent space for each pair of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c57f54-945f-4677-9c8b-978ce14e2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "d_matrix = manhattan_distances(features)\n",
    "d_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f7445-2f27-49ae-93fd-a16fe47b40ed",
   "metadata": {},
   "source": [
    "Then, we can plot the matrix nicely, here we only plot 10 pair because the matrix can get too big really quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c99f869-2214-41bc-855c-83ca4ba39aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "print(\"Distance matrix of the first 10 data points...\")\n",
    "distances = d_matrix[:10, :10]\n",
    "\n",
    "mask = np.triu(np.ones_like(distances, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(distances, mask=mask, cmap=cmap, vmax=distances.max(), center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4f7d1-6fa7-4630-8a79-3d794a66d3ad",
   "metadata": {},
   "source": [
    "Or we can simply plot the whole matrix like we did before;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b035a-7640-45ae-8234-d34581378d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(d_matrix)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b1047-a06c-4179-9c26-398fe057dd45",
   "metadata": {},
   "source": [
    "Note: You can see 3 clusters of point, it's because the data are sorted by targets. If the data was shuffle this structure will not be visible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb7218-729b-4a44-b521-981db6f9cfe8",
   "metadata": {},
   "source": [
    "Now that we have computed the Manhattan Distances, we can now compute the MDS.\n",
    "\n",
    "Note it works better with Manhattan distances than the euclidean distance, it's most likely a high dimension problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8921bd-2f97-4758-972c-cfdb3291bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "mds = MDS(n_components=3,\n",
    "          normalized_stress=False,\n",
    "          metric=True,\n",
    "          dissimilarity=\"precomputed\",\n",
    "          random_state=2024,\n",
    "          eps=1e-9)\n",
    "mds_3d = mds.fit_transform(d_matrix)\n",
    "visualize_3d(mds_3d, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a36ca-15bf-4cbc-bd83-5829cf043357",
   "metadata": {},
   "source": [
    "We can print the \"stress\", which is the sum of squared distance of the disparities and the distances for all constrained points. if you put `normalized_stress = True` and `metric = False`, you will have normalized stress in which 0 means \"perfect\" fit, 0.025 excellent, 0.05 good, 0.1 fair, 0.2 poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6e795a-d574-466c-9adb-7ae966b864ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stress:\", mds.stress_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0464d441-2d90-4e17-aeee-4b3f944987a6",
   "metadata": {},
   "source": [
    "Same as before, we can put 2 component instead of 3.\n",
    "\n",
    "In this case we pass directly the features and the **Euclidean distances** will be used. You can compare the difference of the results using the `dissimilarity` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ca41a-79b7-456b-baf5-69c19b0fd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=2, normalized_stress=False, eps=1e-9, random_state=2024, dissimilarity=\"euclidean\")\n",
    "mds_2d = mds.fit_transform(features)\n",
    "visualize_2d(mds_2d, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2bb2bd-9473-4936-9a42-af4d72f0a400",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "\n",
    "Another technic is the t-SNE, meaning **t-distributed Stocashtic Neighbor embedding**. It's a statistical method with the same objectives as the ones before but based on another algorithm.\n",
    "\n",
    "Same as before, we can do this using 2 or 3 components;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe56f35-3c97-41f5-af63-0b7c2b4ceb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne_3d = TSNE(n_components=3, perplexity=10, early_exaggeration=12, learning_rate='auto', init='pca', n_jobs=4).fit_transform(features)\n",
    "visualize_3d(tsne_3d, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d4eb7-fe28-4a6e-8374-e98273131cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_2d = TSNE(n_components=2, perplexity=10, early_exaggeration=12, learning_rate='auto', init='random', n_jobs=4).fit_transform(features)\n",
    "visualize_2d(tsne_2d, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01bb3fb-2790-415d-b0a6-c8986c1ece8e",
   "metadata": {},
   "source": [
    "## UMAP\n",
    "\n",
    "UMAP is one the most recent method, it's a variant of the t-SNE using a [Riemannian](https://en.wikipedia.org/wiki/Riemannian_manifold#Riemannian_metrics_and_Riemannian_manifolds) metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0392efc9-0ffe-4875-8e7f-92e4cc74b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "reducer = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.1, metric='euclidean')\n",
    "umap_3d = reducer.fit_transform(features)\n",
    "visualize_3d(umap_3d, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad674a-3311-452c-ba41-ec957569a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, metric='euclidean')\n",
    "umap_2d = reducer.fit_transform(features)\n",
    "visualize_2d(umap_2d, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b2293-f7d4-406a-8436-2cb270d7408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = reducer = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.1, metric='euclidean')\n",
    "umap_3d = reducer.fit_transform(features)\n",
    "visualize_3d(umap_3d, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff888063-33d8-49f5-b7eb-40dfbed8e4f8",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Load the `iris` dataset and use the technics seen before to correctly classify the data.\n",
    "\n",
    "Add acolumn in the data to add another flag and do it again to get better results.\n",
    "\n",
    "Quantify your cut by the amount of **True Positives** and **False Positives** counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511058a3-bc8c-4427-903f-43e5569ccb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b23f4-5787-4e97-9475-38836ce41923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
